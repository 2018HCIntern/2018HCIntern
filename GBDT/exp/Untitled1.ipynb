{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garb47/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/garb47/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "import time, os, random, sys\n",
    "import math\n",
    "import hyperopt.tpe\n",
    "import hpsklearn.components\n",
    "import hpsklearn.demo_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaf_indices(ensemble, x):\n",
    "    x = x.astype(np.float32)\n",
    "    trees = ensemble.estimators_\n",
    "    n_trees = trees.shape[0]\n",
    "    indices = []\n",
    "\n",
    "    for i in range(n_trees):\n",
    "        tree = trees[i][0].tree_\n",
    "        indices.append(tree.apply(x))\n",
    "\n",
    "    indices = np.column_stack(indices)\n",
    "    return indices\n",
    "\n",
    "def gbdt_lr_train_test(File):\n",
    "    #GBDT\n",
    "    start = time.clock()\n",
    "    train_df, test_df = train_test_split(File, train_size = 0.75, random_state=10)\n",
    "    X_train = train_df.drop(train_df.columns[0], axis=1)\n",
    "    y_train = train_df[train_df.columns[0]]\n",
    "    X_test = test_df.drop(test_df.columns[0], axis=1)\n",
    "    y_test = test_df[test_df.columns[0]]\n",
    "    \n",
    "    '''\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "    gbclf = GradientBoostingClassifier(n_estimators=20, max_depth=4, verbose=0)\n",
    "    tuned_parameter = [{'n_estimators':[20,30,40,50], 'max_depth':[5, 6, 7, 8], 'max_features':[0.3,0.4,0.5]}]\n",
    "    gs_clf = GridSearchCV(gbclf, tuned_parameter, cv=5, scoring='roc_auc')\n",
    "    gs_clf.fit(X_train, y_train)\n",
    "    print('best parameters set found: ')\n",
    "    print(gs_clf.best_params_)\n",
    "    \n",
    "    y_pred_gbdt = gs_clf.predict_proba(X_test)[:, 1]\n",
    "    gbdt_auc = roc_auc_score(y_test, y_pred_gbdt)\n",
    "    print('gbdt auc: %.5f' % gbdt_auc)\n",
    "    \n",
    "    gbclf=GradientBoostingClassifier(**gs_clf.best_params_)\n",
    "    #-------------------------------------------------------------------------------\n",
    "    '''\n",
    "    gbclf = GradientBoostingClassifier(n_estimators=30, max_depth = 7, verbose= 0, random_state=10)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------\n",
    "    \n",
    "    gbclf.fit(X_train,y_train)\n",
    "    leaf = get_leaf_indices\n",
    "\n",
    "    num_leaf= 400\n",
    "    \n",
    "    y_pred=leaf(gbclf,X_train.values)\n",
    "    print(y_pred)\n",
    "    print(y_pred.shape)\n",
    "    X_train_leaves = np.zeros([len(y_pred),len(y_pred[0]) * num_leaf],dtype=np.int64)\n",
    "    for i in range(0,len(y_pred)):\n",
    "        temp = np.arange(len(y_pred[0])) * num_leaf - 1 + np.array(y_pred[i])\n",
    "        X_train_leaves[i][temp] += 1\n",
    "\n",
    "    y_pred=leaf(gbclf,X_test.values)  \n",
    "    X_test_leaves = np.zeros([len(y_pred),len(y_pred[0]) * num_leaf],dtype=np.int64)\n",
    "    for i in range(0,len(y_pred)):\n",
    "        temp = np.arange(len(y_pred[0])) * num_leaf - 1 + np.array(y_pred[i])\n",
    "        X_test_leaves[i][temp] += 1\n",
    "    \n",
    "    \n",
    "    #GBDT+LR\n",
    "    lr = LogisticRegression(penalty='l1', C=0.2)\n",
    "    lr.fit(X_train_leaves, y_train)\n",
    "    y_pred_gbdtlr1 = lr.predict_proba(X_test_leaves)[:,1]\n",
    "    gbdtlr_auc1 = roc_auc_score(y_test, y_pred_gbdtlr1)\n",
    "    print('gbdt+lr auc 1: %.5f' % gbdtlr_auc1)\n",
    "    \n",
    "    lr = LogisticRegression(n_jobs=-1)\n",
    "    X_train_ext = hstack([X_train_leaves, X_train])\n",
    "    lr.fit(X_train_ext, y_train)\n",
    "    X_test_ext = hstack([X_test_leaves, X_test])\n",
    "    y_pred_gbdtlr2 = lr.predict_proba(X_test_ext)[:, 1]\n",
    "    gbdtlr_auc2 = roc_auc_score(y_test, y_pred_gbdtlr2)\n",
    "    print('gbdt+lr auc 2: %.5f' % gbdtlr_auc2)\n",
    "    f_time =time.clock()-start\n",
    "    print('GBDT+LR time taken: %.2f'% f_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example= pd.read_csv('example2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 72  68  72 ..., 117  69 147]\n",
      " [ 72  68  72 ..., 117  20 147]\n",
      " [167 165 177 ..., 200 168 198]\n",
      " ..., \n",
      " [149 146 158 ..., 185 119 166]\n",
      " [103  99 113 ...,  10 135   8]\n",
      " [103 100 119 ...,  83 102  83]]\n",
      "(6531, 30)\n",
      "gbdt+lr auc 1: 0.95018\n",
      "gbdt+lr auc 2: 0.63462\n",
      "GBDT+LR time taken: 4.35\n"
     ]
    }
   ],
   "source": [
    "gbdt_lr_train_test(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
