{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garb47/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/garb47/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: OMP_NUM_THREADS=None =>\n",
      "... If you are using openblas if you are using openblas set OMP_NUM_THREADS=1 or risk subprocess calls hanging indefinitely\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "import time, os, random, sys\n",
    "import math\n",
    "import hyperopt.tpe\n",
    "import hpsklearn.components\n",
    "import hpsklearn.demo_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_lr(File):\n",
    "\n",
    "    start = time.clock()\n",
    "    train_df, test_df = train_test_split(File, train_size = 0.8)\n",
    "    X_train = train_df.drop(train_df.columns[0], axis=1)\n",
    "    y_train = train_df[train_df.columns[0]]\n",
    "    X_test = test_df.drop(test_df.columns[0], axis=1)\n",
    "    y_test = test_df[test_df.columns[0]]\n",
    "    \n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "    # specify your configurations as a dict\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': {'binary_logloss'},\n",
    "        'num_leaves': 63,\n",
    "        'num_trees': 30,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # number of leaves,will be used in feature transformation\n",
    "    num_leaf = 63\n",
    "\n",
    "\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=lgb_train,\n",
    "                   verbose_eval=False)\n",
    "\n",
    "\n",
    "    y_pred = gbm.predict(X_train,pred_leaf=True)\n",
    "\n",
    "    X_train_leaves = np.zeros([len(y_pred),len(y_pred[0]) * num_leaf],dtype=np.int64)\n",
    "    for i in range(0,len(y_pred)):\n",
    "        temp = np.arange(len(y_pred[0])) * num_leaf - 1 + np.array(y_pred[i])\n",
    "        X_train_leaves[i][temp] += 1\n",
    "\n",
    "\n",
    "    y_pred = gbm.predict(X_test,pred_leaf=True)\n",
    "\n",
    "    X_test_leaves = np.zeros([len(y_pred),len(y_pred[0]) * num_leaf],dtype=np.int64)\n",
    "    for i in range(0,len(y_pred)):\n",
    "        temp = np.arange(len(y_pred[0])) * num_leaf - 1 + np.array(y_pred[i])\n",
    "        X_test_leaves[i][temp] += 1\n",
    "\n",
    "\n",
    "    lm = LogisticRegression(penalty='l2',C=0.1) # logestic model construction\n",
    "    lm.fit(X_train_leaves,y_train)  # fitting the data\n",
    "\n",
    "    y_pred_est = lm.predict_proba(X_test_leaves)   # Give the probabilty on each label\n",
    "\n",
    "    gbdtlr_auc1 = roc_auc_score(y_test, y_pred_est[:,1])\n",
    "    print('lightGBDT+LR auc 1: %.5f' % gbdtlr_auc1)\n",
    "    \n",
    "    lr = LogisticRegression(n_jobs=-1)\n",
    "    X_train_ext = hstack([X_train_leaves, X_train])\n",
    "    lr.fit(X_train_ext, y_train)\n",
    "    X_test_ext = hstack([X_test_leaves, X_test])\n",
    "    y_pred_gbdtlr2 = lr.predict_proba(X_test_ext)[:, 1]\n",
    "    gbdtlr_auc2 = roc_auc_score(y_test, y_pred_gbdtlr2)\n",
    "    print('lightGBDT+LR auc 2: %.5f' % gbdtlr_auc2)\n",
    "    f_time =time.clock()-start\n",
    "    print('lightGBDT+LR time taken: %.2f'% f_time)\n",
    "    \n",
    "    #+NB\n",
    "\n",
    "    \n",
    "    start = time.clock()\n",
    "    gnb= GaussianNB()\n",
    "\n",
    "    gnb.fit(X_train_leaves, y_train)\n",
    "    Y_pred_nb=gnb.predict_proba(X_test_leaves)[:,1]\n",
    "    gnb_auc = roc_auc_score(y_test,Y_pred_nb)\n",
    "    print('GBDT + GNB auc: %.5f'% gnb_auc)    \n",
    "    '''    \n",
    "    gnb.fit(X_train_ext, y_train)\n",
    "    y_pred_gnb2=gnb.predict_proba(X_test_ext)[:,1]\n",
    "    gnb_auc2=roc_auc_score(y_test,y_pred_gnb2)\n",
    "    print('GNB auc2: %.5f' % gnb_auc2)\n",
    "    '''\n",
    "    #svc\n",
    "    svc=SVC(probability=True)\n",
    "    svc.fit(X_train_leaves, y_train)\n",
    "    Y_pred_svc=svc.predict_proba(X_test_leaves)[:,1]\n",
    "    svc_auc=roc_auc_score(y_test,Y_pred_svc)\n",
    "    print('GBDT + SVC auc: %.5f' % svc_auc)\n",
    "    \n",
    "    svc.fit(X_train_ext, y_train)\n",
    "    y_pred_svc2=svc.predict_proba(X_test_ext)[:, 1]\n",
    "    svc_auc2=roc_auc_score(y_test,y_pred_svc2)\n",
    "    print('GBDT + SVC auc2: %.5f' % svc_auc2)\n",
    "    \n",
    "    #KNN\n",
    "    knn=KNeighborsClassifier(n_neighbors = 3)\n",
    "    knn.fit(X_train_leaves, y_train)\n",
    "    Y_pred_knn=knn.predict_proba(X_test_leaves)[:,1]\n",
    "    knn_auc=roc_auc_score(y_test,Y_pred_knn)\n",
    "    print('GBDT + KNN auc : %.5f' % knn_auc)\n",
    "    \n",
    "    knn.fit(X_train_ext, y_train)\n",
    "    y_pred_knn2=knn.predict_proba(X_test_ext)[:, 1]\n",
    "    knn_auc2=roc_auc_score(y_test,y_pred_knn2)\n",
    "    print('GBDT + KNN auc2: %.5f' % knn_auc2)\n",
    "    \n",
    "    #perceptron\n",
    "\n",
    "    perceptron = Perceptron()\n",
    "    perceptron.fit(X_train_leaves, y_train)\n",
    "    y_pred_perc=perceptron.predict(X_test_leaves)\n",
    "    perc_auc=roc_auc_score(y_test,y_pred_perc)\n",
    "    print('GBDT + Perceptron auc : %.5f' % perc_auc)\n",
    "    \n",
    "    perceptron.fit(X_train_ext, y_train)\n",
    "    y_pred_perc2=perceptron.predict(X_test_ext)\n",
    "    perc_auc2=roc_auc_score(y_test,y_pred_perc2)\n",
    "    print('GBDT + Perceptron auc2 : %.5f' % perc_auc2 )\n",
    "    \n",
    "    #linear svc\n",
    "\n",
    "    lin = LinearSVC()\n",
    "    lin.fit(X_train_leaves, y_train)\n",
    "    y_pred_lin=lin.predict(X_test_leaves)\n",
    "    lin_auc=roc_auc_score(y_test,y_pred_lin)\n",
    "    print('GBDT + Linear SVC auc : %.5f' % lin_auc)\n",
    "    \n",
    "    lin.fit(X_train_ext, y_train)\n",
    "    y_pred_lin2=lin.predict(X_test_ext)\n",
    "    lin_auc2=roc_auc_score(y_test,y_pred_lin2)\n",
    "    print('GBDT + Linear SVC auc2 : %.5f' % lin_auc2)\n",
    "    \n",
    "    #SGD\n",
    "\n",
    "    sgd = SGDClassifier(loss='log')\n",
    "    sgd.fit(X_train_leaves, y_train)\n",
    "    Y_pred_sgd=sgd.predict_proba(X_test_leaves)[:, 1]\n",
    "    sgd_auc=roc_auc_score(y_test,Y_pred_sgd)\n",
    "    print('GBDT + SGD auc : %.5f' % sgd_auc)\n",
    "    \n",
    "    sgd.fit(X_train_ext, y_train)\n",
    "    Y_pred_sgd2=sgd.predict_proba(X_test_ext)[:, 1]\n",
    "    sgd_auc2=roc_auc_score(y_test,Y_pred_sgd2)\n",
    "    print('GBDT + SGD auc2 : %.5f' % sgd_auc2)\n",
    "    f_time =time.clock()-start\n",
    "    print('GBDT + other classfier time taken: %.2f'% f_time)    \n",
    "    \n",
    "    #XGB\n",
    "    start = time.clock()\n",
    "    xgb=XGBClassifier()\n",
    "    xgb.fit(X_train,y_train)\n",
    "    Y_pred_xgb=xgb.predict_proba(X_test)[:,1]\n",
    "    xgb_auc= roc_auc_score(y_test,Y_pred_xgb)\n",
    "    print('XGB auc : %.5f' % xgb_auc)\n",
    "    \n",
    "    #XGB with leaves\n",
    "\n",
    "    xgb.fit(X_train_leaves,y_train)\n",
    "    Y_pred_xgb=xgb.predict(X_test_leaves)\n",
    "    xgb_auc= roc_auc_score(y_test,Y_pred_xgb)\n",
    "    print('GBDT + XGB auc: %.5f' % xgb_auc)\n",
    "    \n",
    "    #XGB with features ext\n",
    "    \n",
    "    xgb.fit(X_train_ext, y_train)\n",
    "    y_pred_xgb2=xgb.predict(X_test_ext)\n",
    "    xgb_auc2=roc_auc_score(y_test,y_pred_xgb2)\n",
    "    print('GBDT + XGB auc2: %.5f' %xgb_auc2)\n",
    "    \n",
    "    f_time =time.clock()-start\n",
    "    print('XGB time taken: %.2f'% f_time)\n",
    "    \n",
    "    #lightGBM\n",
    "    start = time.clock()\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # specify your configurations as a dict\n",
    "    params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'auc'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "    \n",
    "    }\n",
    "\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=20,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=5,\n",
    "                    verbose_eval=False)\n",
    "\n",
    "\n",
    "\n",
    "    y_pred_lgb = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    # eval\n",
    "    lgb_auc=roc_auc_score(y_test,y_pred_lgb)\n",
    "    print('lightGBM auc : %.5f' % lgb_auc)\n",
    "    \n",
    "    lgb_train=lgb.Dataset(X_train_leaves, y_train)\n",
    "    lgb_eval=lgb.Dataset(X_test_leaves, y_test, reference=lgb_train)\n",
    "    gbm = lgb.train(params,\n",
    "                  lgb_train,\n",
    "                    num_boost_round=20,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=5,\n",
    "                   verbose_eval=False)\n",
    "    y_pred_lgb2 =gbm.predict(X_test_leaves, num_iteration=gbm.best_iteration)\n",
    "    lgb_auc2=roc_auc_score(y_test, y_pred_lgb2)\n",
    "    \n",
    "    print('GBDT + lightGBM auc : %.5f' % lgb_auc2)\n",
    "    \n",
    "    lgb_train=lgb.Dataset(X_train_ext, y_train)\n",
    "    lgb_eval=lgb.Dataset(X_test_ext, y_test, reference=lgb_train)\n",
    "    gbm = lgb.train(params,\n",
    "                  lgb_train,\n",
    "                    num_boost_round=20,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=5,\n",
    "                   verbose_eval=False)\n",
    "    y_pred_lgb3 =gbm.predict(X_test_ext, num_iteration=gbm.best_iteration)\n",
    "    lgb_auc3=roc_auc_score(y_test, y_pred_lgb3)\n",
    "    \n",
    "    print('GBDT + lightGBM auc2 : %.5f' % lgb_auc3)\n",
    "    \n",
    "    \n",
    "    f_time=time.clock()-start\n",
    "    print('lightGBM time taken: %.2f'% f_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.read_csv('example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbdt+lr auc 1: 0.93285\n",
      "gbdt+lr auc 2: 0.66409\n",
      "GBDT+LR time taken: 1.46\n",
      "GBDT + GNB auc: 0.87764\n",
      "GBDT + SVC auc: 0.93378\n",
      "GBDT + SVC auc2: 0.65948\n",
      "GBDT + KNN auc : 0.90291\n",
      "GBDT + KNN auc2: 0.61911\n",
      "GBDT + Perceptron auc : 0.82928\n",
      "GBDT + Perceptron auc2 : 0.50307\n",
      "GBDT + Linear SVC auc : 0.83684\n",
      "GBDT + Linear SVC auc2 : 0.50000\n",
      "GBDT + SGD auc : 0.90660\n",
      "GBDT + SGD auc2 : 0.52948\n",
      "GBDT + other classfier time taken: 272.53\n",
      "XGB auc : 0.93723\n",
      "GBDT + XGB auc: 0.86437\n",
      "GBDT + XGB auc2: 0.86131\n",
      "XGB time taken: 35.44\n",
      "lightGBM auc : 0.93477\n",
      "GBDT + lightGBM auc : 0.92693\n",
      "GBDT + lightGBM auc2 : 0.92838\n",
      "lightGBM time taken: 1.29\n"
     ]
    }
   ],
   "source": [
    "lightgbm_lr(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightGBDT+LR auc 1: 0.93531\n",
      "lightGBDT+LR auc 2: 0.57736\n",
      "lightGBDT+LR time taken: 1.23\n",
      "GBDT + GNB auc: 0.87557\n",
      "GBDT + SVC auc: 0.93586\n",
      "GBDT + SVC auc2: 0.63296\n",
      "GBDT + KNN auc : 0.89698\n",
      "GBDT + KNN auc2: 0.59092\n",
      "GBDT + Perceptron auc : 0.81886\n",
      "GBDT + Perceptron auc2 : 0.54591\n",
      "GBDT + Linear SVC auc : 0.82871\n",
      "GBDT + Linear SVC auc2 : 0.46588\n",
      "GBDT + SGD auc : 0.88501\n",
      "GBDT + SGD auc2 : 0.48370\n",
      "GBDT + other classfier time taken: 273.51\n",
      "XGB auc : 0.93914\n",
      "GBDT + XGB auc: 0.86303\n",
      "GBDT + XGB auc2: 0.86624\n",
      "XGB time taken: 35.62\n",
      "lightGBM auc : 0.93678\n",
      "GBDT + lightGBM auc : 0.93421\n",
      "GBDT + lightGBM auc2 : 0.93481\n",
      "lightGBM time taken: 1.36\n"
     ]
    }
   ],
   "source": [
    "lightgbm_lr(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightGBDT+LR auc 1: 0.93708\n",
      "lightGBDT+LR auc 2: 0.55838\n",
      "lightGBDT+LR time taken: 1.21\n",
      "GBDT + GNB auc: 0.87613\n",
      "GBDT + SVC auc: 0.93309\n",
      "GBDT + SVC auc2: 0.64572\n",
      "GBDT + KNN auc : 0.89276\n",
      "GBDT + KNN auc2: 0.60250\n",
      "GBDT + Perceptron auc : 0.81466\n",
      "GBDT + Perceptron auc2 : 0.50000\n",
      "GBDT + Linear SVC auc : 0.83640\n",
      "GBDT + Linear SVC auc2 : 0.50000\n",
      "GBDT + SGD auc : 0.90296\n",
      "GBDT + SGD auc2 : 0.49807\n",
      "GBDT + other classfier time taken: 272.25\n",
      "XGB auc : 0.93885\n",
      "GBDT + XGB auc: 0.86197\n",
      "GBDT + XGB auc2: 0.86210\n",
      "XGB time taken: 35.98\n",
      "lightGBM auc : 0.92947\n",
      "GBDT + lightGBM auc : 0.93193\n",
      "GBDT + lightGBM auc2 : 0.92949\n",
      "lightGBM time taken: 1.39\n"
     ]
    }
   ],
   "source": [
    "lightgbm_lr(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=pd.read_csv('example2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightGBDT+LR auc 1: 0.95335\n",
      "lightGBDT+LR auc 2: 0.62290\n",
      "lightGBDT+LR time taken: 1.29\n",
      "GBDT + GNB auc: 0.88576\n",
      "GBDT + SVC auc: 0.94700\n",
      "GBDT + SVC auc2: 0.90184\n",
      "GBDT + KNN auc : 0.92145\n",
      "GBDT + KNN auc2: 0.85685\n",
      "GBDT + Perceptron auc : 0.86075\n",
      "GBDT + Perceptron auc2 : 0.33242\n",
      "GBDT + Linear SVC auc : 0.86186\n",
      "GBDT + Linear SVC auc2 : 0.56348\n",
      "GBDT + SGD auc : 0.92780\n",
      "GBDT + SGD auc2 : 0.71571\n",
      "GBDT + other classfier time taken: 344.54\n",
      "XGB auc : 0.94792\n",
      "GBDT + XGB auc: 0.88256\n",
      "GBDT + XGB auc2: 0.87486\n",
      "XGB time taken: 42.65\n",
      "lightGBM auc : 0.94536\n",
      "GBDT + lightGBM auc : 0.94502\n",
      "GBDT + lightGBM auc2 : 0.94892\n",
      "lightGBM time taken: 1.36\n"
     ]
    }
   ],
   "source": [
    "lightgbm_lr(example) # trying with other dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightGBDT+LR auc 1: 0.96087\n",
      "lightGBDT+LR auc 2: 0.61448\n",
      "lightGBDT+LR time taken: 1.05\n",
      "GBDT + GNB auc: 0.89922\n",
      "GBDT + SVC auc: 0.95530\n",
      "GBDT + SVC auc2: 0.91675\n",
      "GBDT + KNN auc : 0.93072\n",
      "GBDT + KNN auc2: 0.88133\n",
      "GBDT + Perceptron auc : 0.77934\n",
      "GBDT + Perceptron auc2 : 0.50323\n",
      "GBDT + Linear SVC auc : 0.87976\n",
      "GBDT + Linear SVC auc2 : 0.55235\n",
      "GBDT + SGD auc : 0.92308\n",
      "GBDT + SGD auc2 : 0.49132\n",
      "GBDT + other classfier time taken: 343.14\n",
      "XGB auc : 0.95947\n",
      "GBDT + XGB auc: 0.88968\n",
      "GBDT + XGB auc2: 0.88518\n",
      "XGB time taken: 44.83\n",
      "lightGBM auc : 0.95718\n",
      "GBDT + lightGBM auc : 0.95651\n",
      "GBDT + lightGBM auc2 : 0.95730\n",
      "lightGBM time taken: 1.47\n"
     ]
    }
   ],
   "source": [
    "lightgbm_lr(example)# trying with other dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightGBDT+LR auc 1: 0.95171\n",
      "lightGBDT+LR auc 2: 0.61480\n",
      "lightGBDT+LR time taken: 1.14\n",
      "GBDT + GNB auc: 0.87419\n",
      "GBDT + SVC auc: 0.94519\n",
      "GBDT + SVC auc2: 0.91042\n",
      "GBDT + KNN auc : 0.91728\n",
      "GBDT + KNN auc2: 0.87366\n",
      "GBDT + Perceptron auc : 0.84676\n",
      "GBDT + Perceptron auc2 : 0.69394\n",
      "GBDT + Linear SVC auc : 0.86568\n",
      "GBDT + Linear SVC auc2 : 0.77678\n",
      "GBDT + SGD auc : 0.92247\n",
      "GBDT + SGD auc2 : 0.45208\n",
      "GBDT + other classfier time taken: 340.66\n",
      "XGB auc : 0.94640\n"
     ]
    }
   ],
   "source": [
    "lightgbm_lr(example)# trying with other dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
